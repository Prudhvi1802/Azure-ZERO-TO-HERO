stages:
  - build
  - test
  - code-quality
  - docker-build
  - security-scan
  - deploy

variables:
  # Maven configuration
  MAVEN_OPTS: "-Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository"
  MAVEN_CLI_OPTS: "--batch-mode --errors --fail-at-end --show-version"
  
  # Docker configuration
  DOCKER_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA
  DOCKER_IMAGE_LATEST: $CI_REGISTRY_IMAGE:latest
  
  # Kubernetes configuration
  KUBERNETES_NAMESPACE: "production"
  DEPLOYMENT_NAME: "my-microservice"
  CONTAINER_NAME: "app"

# Cache Maven dependencies
cache:
  paths:
    - .m2/repository/
  key:
    files:
      - pom.xml

# Build stage - Compile and package with Maven
build:
  stage: build
  image: maven:3.8-openjdk-17
  script:
    - echo "Building application with Maven..."
    - mvn $MAVEN_CLI_OPTS clean package -DskipTests
    - echo "Build completed successfully"
  artifacts:
    paths:
      - target/*.jar
    expire_in: 1 hour
    reports:
      junit:
        - target/surefire-reports/TEST-*.xml
  tags:
    - on-premise-runner
  only:
    - main
    - develop
    - merge_requests

# Test stage - Run unit tests
test:
  stage: test
  image: maven:3.8-openjdk-17
  script:
    - echo "Running unit tests..."
    - mvn $MAVEN_CLI_OPTS test
    - echo "Tests completed"
  coverage: '/Total.*?([0-9]{1,3})%/'
  artifacts:
    when: always
    reports:
      junit:
        - target/surefire-reports/TEST-*.xml
      coverage_report:
        coverage_format: cobertura
        path: target/site/cobertura/coverage.xml
  tags:
    - on-premise-runner
  only:
    - main
    - develop
    - merge_requests

# Code quality stage - SonarQube analysis
sonarqube-check:
  stage: code-quality
  image: maven:3.8-openjdk-17
  script:
    - echo "Running SonarQube analysis..."
    - mvn $MAVEN_CLI_OPTS verify sonar:sonar
      -Dsonar.projectKey=$SONAR_PROJECT_KEY
      -Dsonar.projectName="$CI_PROJECT_NAME"
      -Dsonar.host.url=$SONAR_HOST_URL
      -Dsonar.login=$SONAR_TOKEN
      -Dsonar.qualitygate.wait=true
    - echo "SonarQube analysis completed"
  allow_failure: true
  tags:
    - on-premise-runner
  only:
    - main
    - develop
    - merge_requests
  except:
    variables:
      - $CI_COMMIT_MESSAGE =~ /\[skip-sonar\]/

# Docker build stage - Build and push Docker image
docker-build:
  stage: docker-build
  image: docker:24-dind
  services:
    - docker:24-dind
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
  before_script:
    - echo "Logging into GitLab Container Registry..."
    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY
  script:
    - echo "Building Docker image..."
    - docker build -t $DOCKER_IMAGE -t $DOCKER_IMAGE_LATEST .
    - echo "Pushing Docker image to registry..."
    - docker push $DOCKER_IMAGE
    - docker push $DOCKER_IMAGE_LATEST
    - echo "Docker image pushed successfully"
  tags:
    - on-premise-runner
  only:
    - main
    - develop

# Security scan stage - Trivy vulnerability scanning
trivy-scan:
  stage: security-scan
  image: aquasec/trivy:latest
  variables:
    TRIVY_NO_PROGRESS: "true"
    TRIVY_CACHE_DIR: ".trivycache/"
  before_script:
    - echo "Preparing Trivy scanner..."
  script:
    - echo "Scanning Docker image for vulnerabilities..."
    - trivy image --exit-code 0 --severity LOW,MEDIUM --no-progress $DOCKER_IMAGE
    - trivy image --exit-code 0 --severity HIGH,CRITICAL --no-progress $DOCKER_IMAGE
    - echo "Security scan completed"
  cache:
    paths:
      - .trivycache/
  artifacts:
    when: always
    reports:
      container_scanning: gl-container-scanning-report.json
  allow_failure: true
  tags:
    - on-premise-runner
  only:
    - main
    - develop

# Deploy stage - Deploy to Kubernetes cluster
deploy-production:
  stage: deploy
  image: bitnami/kubectl:latest
  before_script:
    - echo "Configuring kubectl..."
    - mkdir -p ~/.kube
    - echo "$KUBE_CONFIG" | base64 -d > ~/.kube/config
    - chmod 600 ~/.kube/config
    - kubectl version --client
    - kubectl cluster-info
  script:
    - echo "Deploying to Kubernetes cluster..."
    - |
      # Update deployment image
      kubectl set image deployment/$DEPLOYMENT_NAME \
        $CONTAINER_NAME=$DOCKER_IMAGE \
        -n $KUBERNETES_NAMESPACE
    - echo "Waiting for rollout to complete..."
    - kubectl rollout status deployment/$DEPLOYMENT_NAME -n $KUBERNETES_NAMESPACE --timeout=5m
    - echo "Checking deployment status..."
    - kubectl get deployment $DEPLOYMENT_NAME -n $KUBERNETES_NAMESPACE
    - kubectl get pods -n $KUBERNETES_NAMESPACE -l app=$DEPLOYMENT_NAME
    - echo "Deployment completed successfully!"
  environment:
    name: production
    url: http://your-app-url.com
    on_stop: stop-production
  tags:
    - on-premise-runner
  only:
    - main
  when: manual

# Rollback deployment if needed
rollback-production:
  stage: deploy
  image: bitnami/kubectl:latest
  before_script:
    - echo "Configuring kubectl..."
    - mkdir -p ~/.kube
    - echo "$KUBE_CONFIG" | base64 -d > ~/.kube/config
    - chmod 600 ~/.kube/config
  script:
    - echo "Rolling back deployment..."
    - kubectl rollout undo deployment/$DEPLOYMENT_NAME -n $KUBERNETES_NAMESPACE
    - kubectl rollout status deployment/$DEPLOYMENT_NAME -n $KUBERNETES_NAMESPACE --timeout=5m
    - echo "Rollback completed!"
  tags:
    - on-premise-runner
  only:
    - main
  when: manual

# Stop production environment (for cleanup)
stop-production:
  stage: deploy
  image: bitnami/kubectl:latest
  before_script:
    - mkdir -p ~/.kube
    - echo "$KUBE_CONFIG" | base64 -d > ~/.kube/config
  script:
    - echo "Scaling down deployment..."
    - kubectl scale deployment/$DEPLOYMENT_NAME --replicas=0 -n $KUBERNETES_NAMESPACE
  environment:
    name: production
    action: stop
  tags:
    - on-premise-runner
  only:
    - main
  when: manual
